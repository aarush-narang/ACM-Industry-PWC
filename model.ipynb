{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "98e0e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipaddress\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Training libs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b04d0188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature groups\n",
    "anomaly_detection_input_features = [\n",
    "    \"account_id\",                 # Unique identifier for the user account\n",
    "    \"transaction_id\",             # Unique identifier for the transaction (dropped in training)\n",
    "\n",
    "    \"date\",                       # (String) Transaction date (YYYY-MM-DD)\n",
    "    \"hour_of_day\",                # (Integer) Hour of day when tx occurred (0–23)\n",
    "    \"minute_of_hour\",             # (Integer) Minute of the hour when tx occurred (0–59)\n",
    "    \"day_of_week\",                # (Integer) Day of week (0=Mon ... 6=Sun)\n",
    "    \"is_weekend\",                 # (Boolean) True if tx on Saturday or Sunday\n",
    "    \"is_holiday\",                 # (Boolean) True if tx date is a public holiday\n",
    "    \"days_since_last_holiday\",    # (Integer) Days since the most recent public holiday\n",
    "\n",
    "    \"amount\",                     # (Float) Transaction amount\n",
    "    \"amount_to_avg_ratio\",        # (Float) amount / rolling_mean_3mo_amount\n",
    "    \"amount_percentile\",          # (Float) Percentile rank in user’s historical amounts\n",
    "    \"transaction_type\",           # (String) Credit vs Debit\n",
    "    \"transaction_duration\",       # (Integer) Time in seconds from initiation to settlement\n",
    "    \"is_recurring\",               # (Boolean) Recurring (True) vs one-off (False)\n",
    "    \n",
    "    \"new_payee_flag\",             # (Boolean) True if merchant is new for user\n",
    "    \"avg_unique_merchants_7d\",    # (Float) Avg. number of new merchants in 7-day window\n",
    "\n",
    "    \"rolling_mean_3mo_amount\",    # (Float) 3-month trailing mean transaction amount\n",
    "    \"rolling_std_3mo_amount\",     # (Float) 3-month trailing std deviation of amounts\n",
    "    \"time_since_last_tx_secs\",    # (Integer) Seconds since previous transaction\n",
    "    \"tx_count_last_1h\",           # (Integer) Number of transactions in the last 1 hour\n",
    "    \"tx_count_last_7d\",           # (Integer) Number of transactions in the last 7 days\n",
    "    \"tx_sum_last_24h\",            # (Float) Sum of amounts in the last 24 hours\n",
    "\n",
    "    \"location\",                   # (String) City or ZIP code of transaction\n",
    "    \"is_new_device\",              # (Boolean) True if this device fingerprint is new for user\n",
    "    \"device_age_days\",            # (Integer) Days since device first seen\n",
    "    \"login_attempt_count\",        # (Integer) Number of login attempts in the last 24 hours\n",
    "    \"age\",                        # (Integer) User’s age in years\n",
    "\n",
    "    # NEW STUFF TO PROCESS\n",
    "    \"account_balance\",            # (Float) User’s account balance at the time of transaction\n",
    "    \"channel\",                    # (String) Channel through which the transaction was made (e.g., ATM, online, etc.)\n",
    "    \"customer_occupation\",        # (String) User’s occupation (e.g., engineer, teacher, etc.)\n",
    "    \"ip_address\",                 # (String) User’s IP address \n",
    "\n",
    "    \n",
    "\n",
    "    # # Removed Features\n",
    "    # \"category\",                   # (String) Transaction category (e.g. \"utilities\", \"salary\")\n",
    "    # \"is_foreign_transaction\",     # (Boolean) True if currency or country differs from user's home region\n",
    "    # \"merchant_name\",              # (String) Vendor or payee name\n",
    "    # \"merchant_category\",          # (String) High-level merchant category (e.g., food, utilities)\n",
    "    # \"browser\",                    # (String) Browser fingerprint\n",
    "    # \"os\",                         # (String) Operating system of the device\n",
    "    # \"ip_blacklist_flag\",          # (Boolean) True if IP found on fraud blacklist\n",
    "    # \"vpn_detected\",               # (Boolean) True if IP linked to known VPN or proxy\n",
    "    # \"account_age_days\",           # (Integer) Days since account or card was opened\n",
    "    # \"past_chargeback_count\",      # (Integer) Number of past chargebacks for the user\n",
    "    # \"user_segment\",               # (String) Segment based on spending behavior (e.g., low, medium, high)\n",
    "    # \"mismatch_billing_shipping\",  # (Boolean) True if billing address not same as shipping address\n",
    "]\n",
    "\n",
    "\n",
    "anomaly_detection_output_features = [\n",
    "    # Output features\n",
    "    \"is_anomaly\", # (Boolean): Flagged by the model as anomalous\n",
    "    \"anomaly_confidence\", # (Float): Model’s confidence (0–1) in that flag\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7842dd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with date parsing\n",
    "df = pd.read_csv(\"assets/transactions.csv\", parse_dates=['TransactionDate', 'PreviousTransactionDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f548792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to match the feature list\n",
    "rename_map = {\n",
    "    'TransactionID': 'transaction_id',\n",
    "    'AccountID': 'account_id',\n",
    "    'TransactionAmount': 'amount',\n",
    "    'TransactionDate': 'date',\n",
    "    'TransactionType': 'transaction_type',\n",
    "    'Location': 'location',\n",
    "    'DeviceID': 'device_id',\n",
    "    'IP Address': 'ip_address',\n",
    "    'MerchantID': 'merchant_id',\n",
    "    'Channel': 'channel',\n",
    "    'CustomerAge': 'age',\n",
    "    'CustomerOccupation': 'customer_occupation',\n",
    "    'TransactionDuration': 'transaction_duration',\n",
    "    'LoginAttempts': 'login_attempt_count',\n",
    "    'AccountBalance': 'account_balance',\n",
    "    'PreviousTransactionDate': 'previous_transaction_date'\n",
    "}\n",
    "\n",
    "df = df.rename(columns=rename_map)\n",
    "\n",
    "# Sort data\n",
    "df = df.sort_values([\"account_id\",\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4887835c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gd/d2vxxv2x60j28jw85n3wg5dw0000gn/T/ipykernel_9993/2141539869.py:87: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_avg_unique_merchants_7d)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess features\n",
    "# Extract date components\n",
    "df[\"hour_of_day\"]    = df[\"date\"].dt.hour\n",
    "df[\"minute_of_hour\"] = df[\"date\"].dt.minute\n",
    "df[\"day_of_week\"]    = df[\"date\"].dt.dayofweek\n",
    "df[\"is_weekend\"]     = df[\"day_of_week\"] >= 5\n",
    "\n",
    "# Holidays\n",
    "years = df[\"date\"].dt.year.unique()\n",
    "hols = USFederalHolidayCalendar().holidays(\n",
    "    start=f\"{years.min()-1}-01-01\",\n",
    "    end=  f\"{years.max()+1}-12-31\"\n",
    ")\n",
    "df[\"is_holiday\"] = df[\"date\"].dt.normalize().isin(hols)\n",
    "\n",
    "def days_since_last(x):\n",
    "    prev = hols[hols < x.normalize()]\n",
    "    return (x.normalize() - prev.max()).days if len(prev) else (x.normalize() - hols.min()).days\n",
    "\n",
    "df[\"days_since_last_holiday\"] = df[\"date\"].apply(days_since_last)\n",
    "\n",
    "# Time since last transaction\n",
    "df[\"time_since_last_tx_secs\"] = (\n",
    "    df.groupby(\"account_id\")[\"date\"]\n",
    "      .diff()\n",
    "      .dt.total_seconds()\n",
    "      .fillna(0)\n",
    "      .astype(int)\n",
    ")\n",
    "\n",
    "# Rolling statistics (per account)\n",
    "r90 = df.groupby(\"account_id\")\\\n",
    "        .rolling(\"90d\", on=\"date\", min_periods=1)[\"amount\"]\n",
    "df[\"rolling_mean_3mo_amount\"] = r90.mean().reset_index(level=0, drop=True)\n",
    "df[\"rolling_std_3mo_amount\"]  = r90.std().reset_index(level=0, drop=True)\n",
    "\n",
    "# Fill NaN values\n",
    "df[[\"rolling_mean_3mo_amount\",\"rolling_std_3mo_amount\"]] = (\n",
    "    df[[\"rolling_mean_3mo_amount\",\"rolling_std_3mo_amount\"]].fillna(0)\n",
    ")\n",
    "\n",
    "# Velocity features\n",
    "w1h_exc  = df.groupby(\"account_id\").rolling(\"1h\",  on=\"date\", closed=\"left\", min_periods=0)[\"amount\"]\n",
    "w7d_exc  = df.groupby(\"account_id\").rolling(\"7d\",  on=\"date\", closed=\"left\", min_periods=0)[\"amount\"]\n",
    "w24h_exc = df.groupby(\"account_id\").rolling(\"24h\", on=\"date\", closed=\"left\", min_periods=0)[\"amount\"]\n",
    "\n",
    "df[\"tx_count_last_1h\"] = w1h_exc.count().reset_index(level=0, drop=True)\n",
    "df[\"tx_count_last_7d\"] = w7d_exc.count().reset_index(level=0, drop=True)\n",
    "df[\"tx_sum_last_24h\"] = w24h_exc.sum().reset_index(level=0, drop=True)\n",
    "\n",
    "# Fill NaN values\n",
    "df[[\"tx_count_last_1h\",\"tx_count_last_7d\",\"tx_sum_last_24h\"]] = (\n",
    "    df[[\"tx_count_last_1h\",\"tx_count_last_7d\",\"tx_sum_last_24h\"]]\n",
    "    .fillna(0)\n",
    ")\n",
    "df[[\"tx_count_last_1h\",\"tx_count_last_7d\"]] = df[[\"tx_count_last_1h\",\"tx_count_last_7d\"]].astype(int)\n",
    "\n",
    "\n",
    "# Derived ratios & percentiles\n",
    "df[\"amount_to_avg_ratio\"] = (\n",
    "    df[\"amount\"] / df[\"rolling_mean_3mo_amount\"].replace(0, np.nan)\n",
    ").fillna(1)\n",
    "df[\"amount_percentile\"] = df.groupby(\"account_id\")[\"amount\"].rank(pct=True)\n",
    "\n",
    "# Recurrence & novelty\n",
    "df[\"is_recurring\"]   = df.duplicated(\n",
    "    subset=[\"account_id\",\"merchant_id\",\"amount\"], keep=False\n",
    ")\n",
    "df[\"new_payee_flag\"] = df.groupby(\n",
    "    [\"account_id\",\"merchant_id\"]\n",
    ").cumcount() == 0\n",
    "\n",
    "# Merchant novelty\n",
    "def compute_avg_unique_merchants_7d(group):\n",
    "    merchants = group['merchant_id']\n",
    "    dates = group['date']\n",
    "    vals = []\n",
    "    for current_time in dates:\n",
    "        window_start = current_time - pd.Timedelta(days=7)\n",
    "        mask = (dates >= window_start) & (dates <= current_time)\n",
    "        vals.append(merchants[mask].nunique() / 7)\n",
    "    return pd.Series(vals, index=group.index)\n",
    "\n",
    "df = df.sort_values(['account_id','date'])\n",
    "df['avg_unique_merchants_7d'] = (\n",
    "    df.groupby('account_id')\n",
    "      .apply(compute_avg_unique_merchants_7d)\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# Device context\n",
    "df['is_new_device']   = df.groupby(\n",
    "    ['account_id', 'device_id']\n",
    ").cumcount() == 0\n",
    "df['device_age_days'] = (\n",
    "    df['date'] - df.groupby('device_id')['date'].transform('min')\n",
    ").dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bc238e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_id</th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>date</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>minute_of_hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>days_since_last_holiday</th>\n",
       "      <th>amount</th>\n",
       "      <th>...</th>\n",
       "      <th>tx_sum_last_24h</th>\n",
       "      <th>location</th>\n",
       "      <th>is_new_device</th>\n",
       "      <th>device_age_days</th>\n",
       "      <th>login_attempt_count</th>\n",
       "      <th>age</th>\n",
       "      <th>account_balance</th>\n",
       "      <th>channel</th>\n",
       "      <th>customer_occupation</th>\n",
       "      <th>ip_address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>AC00001</td>\n",
       "      <td>TX001313</td>\n",
       "      <td>2023-09-15 17:00:20</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "      <td>47.79</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Denver</td>\n",
       "      <td>True</td>\n",
       "      <td>186</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1649.92</td>\n",
       "      <td>Branch</td>\n",
       "      <td>Student</td>\n",
       "      <td>59.12.96.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>AC00001</td>\n",
       "      <td>TX002017</td>\n",
       "      <td>2023-11-14 16:56:34</td>\n",
       "      <td>16</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>212.97</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>4180.40</td>\n",
       "      <td>Online</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>45.241.13.208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2120</th>\n",
       "      <td>AC00002</td>\n",
       "      <td>TX002121</td>\n",
       "      <td>2023-01-10 16:00:32</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>476.99</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1154.48</td>\n",
       "      <td>Online</td>\n",
       "      <td>Student</td>\n",
       "      <td>113.137.153.101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AC00002</td>\n",
       "      <td>TX000021</td>\n",
       "      <td>2023-02-28 16:36:58</td>\n",
       "      <td>16</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>59.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>5750.89</td>\n",
       "      <td>Branch</td>\n",
       "      <td>Retired</td>\n",
       "      <td>116.44.12.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476</th>\n",
       "      <td>AC00002</td>\n",
       "      <td>TX001477</td>\n",
       "      <td>2023-05-05 16:35:44</td>\n",
       "      <td>16</td>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>74</td>\n",
       "      <td>12.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>El Paso</td>\n",
       "      <td>True</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>6420.47</td>\n",
       "      <td>Branch</td>\n",
       "      <td>Doctor</td>\n",
       "      <td>93.160.83.196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     account_id transaction_id                date  hour_of_day  \\\n",
       "1312    AC00001       TX001313 2023-09-15 17:00:20           17   \n",
       "2016    AC00001       TX002017 2023-11-14 16:56:34           16   \n",
       "2120    AC00002       TX002121 2023-01-10 16:00:32           16   \n",
       "20      AC00002       TX000021 2023-02-28 16:36:58           16   \n",
       "1476    AC00002       TX001477 2023-05-05 16:35:44           16   \n",
       "\n",
       "      minute_of_hour  day_of_week  is_weekend  is_holiday  \\\n",
       "1312               0            4       False       False   \n",
       "2016              56            1       False       False   \n",
       "2120               0            1       False       False   \n",
       "20                36            1       False       False   \n",
       "1476              35            4       False       False   \n",
       "\n",
       "      days_since_last_holiday  amount  ...  tx_sum_last_24h     location  \\\n",
       "1312                       11   47.79  ...              0.0       Denver   \n",
       "2016                        4  212.97  ...              0.0      Atlanta   \n",
       "2120                        8  476.99  ...              0.0    San Diego   \n",
       "20                          8   59.32  ...              0.0  Los Angeles   \n",
       "1476                       74   12.62  ...              0.0      El Paso   \n",
       "\n",
       "     is_new_device  device_age_days  login_attempt_count  age  \\\n",
       "1312          True              186                    1   25   \n",
       "2016          True                0                    1   59   \n",
       "2120          True                0                    1   23   \n",
       "20            True                0                    1   71   \n",
       "1476          True               66                    1   33   \n",
       "\n",
       "      account_balance  channel  customer_occupation       ip_address  \n",
       "1312          1649.92   Branch              Student      59.12.96.11  \n",
       "2016          4180.40   Online             Engineer    45.241.13.208  \n",
       "2120          1154.48   Online              Student  113.137.153.101  \n",
       "20            5750.89   Branch              Retired    116.44.12.250  \n",
       "1476          6420.47   Branch               Doctor    93.160.83.196  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final Cleanup\n",
    "final_features = [c for c in anomaly_detection_input_features if c in df.columns]\n",
    "df = df[final_features]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a90af8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the cleaned dataframe to a new CSV file\n",
    "df.to_csv(\"assets/transactions_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5e691194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical features to numerical\n",
    "# Load preprocessed feature CSV\n",
    "df = pd.read_csv(\"assets/transactions_cleaned.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# Drop non-features\n",
    "cols_to_drop = [\"account_id\", \"transaction_id\", \"date\"]\n",
    "df_model = df.drop(columns=cols_to_drop)\n",
    "\n",
    "# One hot encoding for categorical features\n",
    "# One-hot encode \"location\" column and join into df_model\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "loc_mat = ohe.fit_transform(df_model[[\"location\"]])\n",
    "loc_cols = [f\"loc_{c}\" for c in ohe.categories_[0]]\n",
    "df_loc = pd.DataFrame(loc_mat, columns=loc_cols, index=df_model.index)\n",
    "\n",
    "df_model = pd.concat([\n",
    "    df_model.drop(columns=[\"location\"]), # Drop original location column\n",
    "    df_loc\n",
    "], axis=1)\n",
    "\n",
    "# One-hot encode \"channel\" column and join into df_model\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "channel_mat = ohe.fit_transform(df_model[[\"channel\"]])\n",
    "channel_cols = [f\"channel_{c}\" for c in ohe.categories_[0]]\n",
    "df_channel = pd.DataFrame(channel_mat, columns=channel_cols, index=df_model.index)\n",
    "\n",
    "df_model = pd.concat([\n",
    "    df_model.drop(columns=[\"channel\"]), # Drop original channel column\n",
    "    df_channel\n",
    "], axis=1)\n",
    "\n",
    "# One-hot encode \"customer_occupation\" column and join into df_model\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "occupation_mat = ohe.fit_transform(df_model[[\"customer_occupation\"]])\n",
    "occupation_cols = [f\"occupation_{c}\" for c in ohe.categories_[0]]\n",
    "df_occupation = pd.DataFrame(occupation_mat, columns=occupation_cols, index=df_model.index)\n",
    "\n",
    "df_model = pd.concat([\n",
    "    df_model.drop(columns=[\"customer_occupation\"]), # Drop original occupation column\n",
    "    df_occupation\n",
    "], axis=1)\n",
    "\n",
    "# Other categorical columns\n",
    "# Convert any boolean columns to 0/1\n",
    "bool_cols = df_model.select_dtypes(include=\"bool\").columns\n",
    "for c in bool_cols:\n",
    "    df_model[c] = df_model[c].astype(int)\n",
    "\n",
    "# Convert credit/debit to binary\n",
    "df_model['transaction_type'] = df_model['transaction_type'].map({\n",
    "    'Credit': 1,\n",
    "    'Debit': 0\n",
    "})\n",
    "\n",
    "# Convert IP address to numerical format\n",
    "df_model['ip_address'] = df_model['ip_address'].apply(\n",
    "    lambda x: int(ipaddress.ip_address(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "552f27b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling and Training\n",
    "# Prepare numeric matrix and scale\n",
    "X = df_model.astype(float)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train IsolationForest\n",
    "iso = IsolationForest(\n",
    "    n_estimators=500,\n",
    "    max_samples=1.0,      \n",
    "    max_features=1.0,\n",
    "    bootstrap=True,\n",
    "    contamination=0.05,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=0,\n",
    ")\n",
    "iso.fit(X_scaled)\n",
    "\n",
    "# Predict & score\n",
    "labels     = iso.predict(X_scaled)         # 1 = normal, -1 = anomaly\n",
    "raw_scores = iso.score_samples(X_scaled)   # higher = more normal\n",
    "\n",
    "df[\"is_anomaly\"] = labels == -1\n",
    "\n",
    "inv = -raw_scores\n",
    "df[\"anomaly_confidence\"] = (inv - inv.min()) / (inv.max() - inv.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d1a587b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     transaction_id  amount  is_anomaly  anomaly_confidence\n",
      "0          TX001313   47.79       False            0.195565\n",
      "1          TX002017  212.97       False            0.187227\n",
      "2          TX002121  476.99       False            0.150111\n",
      "3          TX000021   59.32       False            0.165020\n",
      "4          TX001477   12.62       False            0.286291\n",
      "...             ...     ...         ...                 ...\n",
      "2507       TX001943  837.29       False            0.246154\n",
      "2508       TX002055  156.86       False            0.124883\n",
      "2509       TX001716  225.93       False            0.204888\n",
      "2510       TX002375  129.62       False            0.292553\n",
      "2511       TX002350  134.55       False            0.391922\n",
      "\n",
      "[2512 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Save/print results\n",
    "important_head_cols = [\n",
    "    \"transaction_id\", \"amount\", \n",
    "    \"is_anomaly\", \"anomaly_confidence\"\n",
    "]\n",
    "print(df[important_head_cols])\n",
    "df.to_csv(\"assets/transactions_with_anomalies.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acm-industry-pwc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
