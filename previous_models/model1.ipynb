{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e0e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipaddress\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Training libs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b04d0188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature groups\n",
    "anomaly_detection_input_features = [\n",
    "    # Non-feature columns\n",
    "    \"account_id\",                 # Unique identifier for the user account\n",
    "    \"transaction_id\",             # Unique identifier for the transaction (dropped in training)\n",
    "    \"date\",                       # (String) Transaction date (YYYY-MM-DD)\n",
    "\n",
    "    # Feature columns\n",
    "    \"amount\",                     # (Float) Transaction amount\n",
    "    \"merchant_name\",              # (String) Name of the merchant (OHEd)\n",
    "    \"transaction_type\",           # (String) Credit vs Debit (OHEd)\n",
    "    \"location\",                   # (String) City or ZIP code of transaction location (OHEd)\n",
    "    \"login_attempts\",        # (Integer) Number of login attempts before transaction\n",
    "    \"transaction_duration\",       # (Integer) Time in seconds from initiation to settlement\n",
    "    \"is_weekend\",                 # (Boolean) True if tx on Saturday or Sunday\n",
    "    \"day_of_week\",                # (Integer) Day of week (0=Mon ... 6=Sun)\"\n",
    "    \"recurrence_flag\",               # (Boolean) Recurring (True) vs one-off (False)\n",
    "    \"rolling_mean_3mo_amount\",    # (Float) 3-month trailing mean transaction amount\n",
    "    \"rolling_std_3mo_amount\",     # (Float) 3-month trailing std deviation of amounts\n",
    "    \"time_since_last_tx_days\",    # (Float) Days since previous transaction\n",
    "\n",
    "    # Derived features\n",
    "    \"is_holiday\",                 # (Bool`ean) True if tx date is a public holiday\n",
    "    \"amount_percentile\",          # (Float) Percentile rank in user’s historical amounts\n",
    "]\n",
    "\n",
    "anomaly_detection_output_features = [\n",
    "    # Output features\n",
    "    \"is_anomaly\", # (Boolean): Flagged by the model as anomalous\n",
    "    \"anomaly_confidence\", # (Float): Model’s confidence (0–1) in that flag\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7842dd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with date parsing\n",
    "df = pd.read_csv(\"assets/unlabeled_data/transactions.csv\", parse_dates=['TransactionDate', 'PreviousTransactionDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f548792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to match the feature list\n",
    "rename_map = {\n",
    "    'TransactionID': 'transaction_id',\n",
    "    'AccountID': 'account_id',\n",
    "    'TransactionAmount': 'amount',\n",
    "    'TransactionDate': 'date',\n",
    "    'TransactionType': 'transaction_type',\n",
    "    'Location': 'location',\n",
    "    'DeviceID': 'device_id',\n",
    "    'IP Address': 'ip_address',\n",
    "    'MerchantID': 'merchant_name',\n",
    "    'Channel': 'channel',\n",
    "    'CustomerAge': 'age',\n",
    "    'CustomerOccupation': 'customer_occupation',\n",
    "    'TransactionDuration': 'transaction_duration',\n",
    "    'LoginAttempts': 'login_attempts',\n",
    "    'AccountBalance': 'account_balance',\n",
    "    'PreviousTransactionDate': 'previous_transaction_date'\n",
    "}\n",
    "\n",
    "df = df.rename(columns=rename_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4887835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess features\n",
    "def preprocess(v_df):\n",
    "    #* Sort Data\n",
    "    v_df = v_df.sort_values([\"account_id\",\"date\"])\n",
    "\n",
    "    #* Extract date components\n",
    "    v_df[\"day_of_week\"]    = v_df[\"date\"].dt.dayofweek\n",
    "    v_df[\"is_weekend\"]     = v_df[\"day_of_week\"] >= 5\n",
    "\n",
    "    #* Rolling statistics\n",
    "    if \"rolling_mean_3mo_amount\" not in v_df.columns:\n",
    "        v_df[\"rolling_mean_3mo_amount\"] = (\n",
    "            v_df.groupby(\"account_id\")[\"amount\"]\n",
    "            .transform(lambda x: x.rolling(90, min_periods=1).mean())\n",
    "            .fillna(0)\n",
    "        )\n",
    "    if \"rolling_std_3mo_amount\" not in v_df.columns:\n",
    "        v_df[\"rolling_std_3mo_amount\"] = (\n",
    "            v_df.groupby(\"account_id\")[\"amount\"]\n",
    "            .transform(lambda x: x.rolling(90, min_periods=1).std())\n",
    "            .fillna(0)\n",
    "        )\n",
    "\n",
    "    #* Recurrence\n",
    "    if \"recurrence_flag\" not in v_df.columns:\n",
    "        v_df[\"recurrence_flag\"]   = v_df.duplicated(\n",
    "            subset=[\"account_id\",\"merchant_name\",\"amount\"], keep=False\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d2447b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive features\n",
    "def derive(v_df):\n",
    "    #* Holidays\n",
    "    years = v_df[\"date\"].dt.year.unique()\n",
    "    hols = USFederalHolidayCalendar().holidays(\n",
    "        start=f\"{years.min()-1}-01-01\",\n",
    "        end=  f\"{years.max()+1}-12-31\"\n",
    "    )\n",
    "    v_df[\"is_holiday\"] = v_df[\"date\"].dt.normalize().isin(hols)\n",
    "\n",
    "    #* Percentiles\n",
    "    v_df[\"amount_percentile\"] = v_df.groupby(\"account_id\")[\"amount\"].rank(pct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc238e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Cleanup\n",
    "preprocess(df)\n",
    "derive(df)\n",
    "\n",
    "final_features = [c for c in anomaly_detection_input_features if c in df.columns]\n",
    "df = df[final_features]\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a90af8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the cleaned dataframe to a new CSV file\n",
    "df.to_csv(\"assets/unlabeled_data/transactions_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67f3e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(v_df, cols_to_encode):\n",
    "    # One-hot encode categorical columns and join into v_df\n",
    "    for col in cols_to_encode:\n",
    "        ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "        loc_mat = ohe.fit_transform(v_df[[col]])\n",
    "        loc_cols = [f\"{col}_{c}\" for c in ohe.categories_[0]]\n",
    "        df_loc = pd.DataFrame(loc_mat, columns=loc_cols, index=v_df.index)\n",
    "\n",
    "        # drop the original column and join the new columns\n",
    "        v_df = v_df.drop(columns=[col])\n",
    "        v_df = pd.concat([v_df, df_loc], axis=1)\n",
    "    \n",
    "    return v_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e691194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical features to numerical\n",
    "# Load preprocessed feature CSV\n",
    "df = pd.read_csv(\"assets/unlabeled_data/transactions_cleaned.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "#! Drop non-features\n",
    "cols_to_drop = [\"account_id\", \"transaction_id\", \"date\"]\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "# One hot encoding for categorical features\n",
    "df = encode(df, [\"merchant_name\", \"location\"])\n",
    "\n",
    "# Other categorical columns\n",
    "# Convert any boolean columns to 0/1\n",
    "bool_cols = df.select_dtypes(include=\"bool\").columns\n",
    "for c in bool_cols:\n",
    "    df[c] = df[c].astype(int)\n",
    "\n",
    "# Convert credit/debit to binary\n",
    "df['transaction_type'] = df['transaction_type'].map({\n",
    "    'Credit': 1,\n",
    "    'Debit': 0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "552f27b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling and Training\n",
    "# Prepare numeric matrix and scale\n",
    "X = df.astype(float)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train IsolationForest\n",
    "iso = IsolationForest(\n",
    "    n_estimators=500,\n",
    "    max_samples=1.0,      \n",
    "    max_features=1.0,\n",
    "    bootstrap=True,\n",
    "    contamination=0.05,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=0,\n",
    ")\n",
    "iso.fit(X_scaled)\n",
    "\n",
    "# Predict & score\n",
    "labels     = iso.predict(X_scaled)         # 1 = normal, -1 = anomaly\n",
    "raw_scores = iso.score_samples(X_scaled)   # higher = more normal\n",
    "\n",
    "df[\"is_anomaly\"] = labels == -1\n",
    "\n",
    "inv = -raw_scores\n",
    "df[\"anomaly_confidence\"] = (inv - inv.min()) / (inv.max() - inv.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1a587b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      amount  is_anomaly  anomaly_confidence\n",
      "0      14.09       False            0.295466\n",
      "1     376.24       False            0.289169\n",
      "2     126.29       False            0.415668\n",
      "3     184.50       False            0.500506\n",
      "4      13.45       False            0.457014\n",
      "...      ...         ...                 ...\n",
      "2507  856.21       False            0.437856\n",
      "2508  251.54       False            0.185300\n",
      "2509   28.63       False            0.220011\n",
      "2510  185.97       False            0.171144\n",
      "2511  243.08       False            0.390393\n",
      "\n",
      "[2512 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Save/print results\n",
    "important_head_cols = [\n",
    "    \"amount\", \n",
    "    \"is_anomaly\", \"anomaly_confidence\"\n",
    "]\n",
    "print(df[important_head_cols])\n",
    "\n",
    "# df.to_csv(\"assets/unlabeled_data/transactions_with_anomalies.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebbe7b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.8883\n",
      "Precision: 0.6122\n",
      "Recall: 0.2381\n",
      "F1 Score: 0.3429\n",
      "ROC AUC Score: 0.6760\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
    "# Calculate Metrics\n",
    "##! Calculate accuracy with the labeled datasets\n",
    "\n",
    "#* Synthetic dataset\n",
    "#* Load the synthetic dataset\n",
    "df_synthetic = pd.read_csv(\"assets/labeled_data/synthetic_labeled_transactions.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "#? If account_id is not in the synthetic dataset, add a column with the same value. This column will be dropped later.\n",
    "if \"account_id\" not in df_synthetic.columns:\n",
    "    df_synthetic[\"account_id\"] = \"account_id\" # [ GMX_user_file: model.ipynb ]\n",
    "\n",
    "##! Preprocess the synthetic dataset\n",
    "preprocess(df_synthetic) # [ GMX_user_file: model.ipynb ]\n",
    "derive(df_synthetic) # [ GMX_user_file: model.ipynb ]\n",
    "\n",
    "#* Make sure all required columns are present\n",
    "missing_cols = [col for col in anomaly_detection_input_features if col not in df_synthetic.columns] # [ GMX_user_file: model.ipynb ]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing columns in synthetic dataset: {missing_cols}\")\n",
    "\n",
    "#* Drop non-feature columns\n",
    "# Keep 'is_anomaly' for evaluation\n",
    "cols_to_drop_features_only = [\"account_id\", \"transaction_id\", \"date\"] # Modified from original\n",
    "df_synthetic_model = df_synthetic.drop(columns=cols_to_drop_features_only) # Modified from original\n",
    "\n",
    "# Store true labels before potentially dropping the column\n",
    "y_true = df_synthetic[\"is_anomaly\"].astype(int) # [ GMX_user_file: model.ipynb ]\n",
    "\n",
    "# Drop the true label column if it's still in df_synthetic_model\n",
    "if \"is_anomaly\" in df_synthetic_model.columns:\n",
    "     df_synthetic_model = df_synthetic_model.drop(columns=[\"is_anomaly\"]) # [ GMX_user_file: model.ipynb ]\n",
    "\n",
    "\n",
    "#* One hot encoding for categorical features\n",
    "df_synthetic_model = encode(df_synthetic_model, [\"merchant_name\", \"location\"]) # [ GMX_user_file: model.ipynb ]\n",
    "\n",
    "#* Convert any boolean columns to 0/1\n",
    "bool_cols = df_synthetic_model.select_dtypes(include=\"bool\").columns # [ GMX_user_file: model.ipynb ]\n",
    "for c in bool_cols:\n",
    "    df_synthetic_model[c] = df_synthetic_model[c].astype(int) # [ GMX_user_file: model.ipynb ]\n",
    "\n",
    "#* Convert credit/debit to binary\n",
    "df_synthetic_model['transaction_type'] = df_synthetic_model['transaction_type'].map({\n",
    "    'Credit': 1,\n",
    "    'Debit': 0\n",
    "}) # [ GMX_user_file: model.ipynb ]\n",
    "\n",
    "#* Ensure columns match the training data (df)\n",
    "# Add missing columns with zeros (OHE encoded columns)\n",
    "# Use the columns from the *scaled* training data `X` before anomaly columns were added\n",
    "train_cols = X.columns # Assuming X is the DataFrame before scaling in cell 11\n",
    "for col in train_cols:\n",
    "    if col not in df_synthetic_model.columns:\n",
    "        df_synthetic_model[col] = 0 # [ GMX_user_file: model.ipynb ]\n",
    "\n",
    "# Remove extra columns not in training data\n",
    "df_synthetic_model = df_synthetic_model[train_cols] # Use train_cols order and selection [ GMX_user_file: model.ipynb ]\n",
    "\n",
    "\n",
    "##! Predict on the synthetic dataset\n",
    "#* Scaling\n",
    "X_synthetic = df_synthetic_model.astype(float) # [ GMX_user_file: model.ipynb ]\n",
    "X_synthetic_scaled = scaler.transform(X_synthetic) # [ GMX_user_file: model.ipynb ]\n",
    "\n",
    "#* Predict & score.\n",
    "labels_synthetic = iso.predict(X_synthetic_scaled)         # 1 = normal, -1 = anomaly [ GMX_user_file: model.ipynb ]\n",
    "raw_scores_synthetic = iso.score_samples(X_synthetic_scaled)   # higher = more normal [ GMX_user_file: model.ipynb ]\n",
    "\n",
    "# Create prediction column: is_anomaly_pred (convert -1 to 1, 1 to 0)\n",
    "y_pred = (labels_synthetic == -1).astype(int) # [ GMX_user_file: model.ipynb ]\n",
    "\n",
    "# Use the negative raw scores for ROC AUC (higher score -> more anomalous)\n",
    "y_scores = -raw_scores_synthetic # [ GMX_user_file: model.ipynb ]\n",
    "\n",
    "\n",
    "##! Calculate and Print Metrics\n",
    "\n",
    "# Testing Accuracy (already calculated in a similar way)\n",
    "# Ensure y_true and y_pred are correctly aligned if df_synthetic index was changed\n",
    "# Re-calculate using sklearn function for consistency\n",
    "test_accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_true, y_pred, zero_division=0) # Handles cases with no predicted positives\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_true, y_pred, zero_division=0) # Handles cases with no actual positives\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0) # Handles cases with no positives in either true or pred\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# ROC AUC Score\n",
    "# Check if there is more than one class in true labels before calculating ROC AUC\n",
    "if len(np.unique(y_true)) > 1:\n",
    "    roc_auc = roc_auc_score(y_true, y_scores)\n",
    "    print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "else:\n",
    "    print(\"ROC AUC Score: Not defined (only one class present in true labels)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acm-industry-pwc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
