{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6886005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, losses, mixed_precision\n",
    "\n",
    "# Scikit-learn helpers\n",
    "from sklearn.compose        import ColumnTransformer\n",
    "from sklearn.preprocessing   import OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition   import TruncatedSVD\n",
    "from sklearn.ensemble        import IsolationForest\n",
    "from sklearn.metrics         import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Reproducibility --------------------------------------------------------\n",
    "RANDOM_STATE = 42\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# --- SPEED + MEMORY TUNING --------------------------------------------------\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")   # fast *and* half-sized tensors\n",
    "# tf.config.optimizer.set_jit(True)   # comment out or delete this line\n",
    "\n",
    "BATCH      = 1024        # 1 k fits easily; raise or lower as needed\n",
    "CACHE_TRAIN = \"./ae_train.cache\"   # on-disk cache files\n",
    "CACHE_VAL   = \"./ae_val.cache\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c63fb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: load & prepare a CSV (+labels)\n",
    "def load_and_prepare(path_csv, path_labels=None):\n",
    "    \"\"\"\n",
    "    Load a transactions CSV and optional labels JSON.\n",
    "    Adds hour_of_day & day_of_week. Supports either\n",
    "    string or numeric amount.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\n",
    "        path_csv,\n",
    "        usecols=[\"id\", \"date\", \"amount\", \"merchant_city\", \"mcc\"]\n",
    "    )\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "    # robust amount cleaning\n",
    "    if pd.api.types.is_numeric_dtype(df[\"amount\"]):\n",
    "        df[\"amount\"] = df[\"amount\"].astype(float)\n",
    "    else:\n",
    "        df[\"amount\"] = (\n",
    "            df[\"amount\"]\n",
    "              .astype(str)\n",
    "              .str.replace(r\"[$,]\", \"\", regex=True)\n",
    "              .astype(float)\n",
    "        )\n",
    "\n",
    "    df = df.rename(columns={\"id\": \"transaction_id\",\n",
    "                            \"merchant_city\": \"location\"})\n",
    "    df[\"hour_of_day\"] = df[\"date\"].dt.hour\n",
    "    df[\"day_of_week\"] = df[\"date\"].dt.weekday\n",
    "\n",
    "    if path_labels is not None:\n",
    "        lbl = (\n",
    "            pd.read_json(path_labels)\n",
    "              .reset_index()\n",
    "              .rename(columns={\"index\": \"transaction_id\"})\n",
    "              .replace({\"target\": {\"Yes\": 1, \"No\": 0}})\n",
    "        )\n",
    "        df = df.merge(lbl, on=\"transaction_id\")\n",
    "\n",
    "    # inject target column if it exists\n",
    "    df_target = pd.read_csv(path_csv)\n",
    "    if \"target\" in df_target.columns:\n",
    "        df[\"target\"] = df_target[\"target\"].astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f85eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ColumnTransformer + SVD reducer\n",
    "cat_cols = [\"location\", \"mcc\"]\n",
    "num_cols = [\"amount\", \"hour_of_day\", \"day_of_week\"]\n",
    "\n",
    "base_pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\",\n",
    "         OneHotEncoder(handle_unknown=\"ignore\",\n",
    "                       sparse_output=True,\n",
    "                       dtype=np.int8),\n",
    "         cat_cols),\n",
    "        (\"num\", StandardScaler(), num_cols)\n",
    "    ],\n",
    "    sparse_threshold=0.3\n",
    ")\n",
    "\n",
    "# Reduce high-dim sparse matrix to 200 dense features\n",
    "svd = TruncatedSVD(\n",
    "        n_components=200,\n",
    "        algorithm=\"randomized\",\n",
    "        n_iter=5,\n",
    "        random_state=RANDOM_STATE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67336803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Isolation Forest on Kaggle\n",
    "kag_csv   = \"../assets/kaggle_data/transactions_data.csv\"\n",
    "kag_lbl   = \"../assets/kaggle_data/train_fraud_labels.json\"\n",
    "\n",
    "train_df  = load_and_prepare(kag_csv, kag_lbl)\n",
    "y_train   = train_df[\"target\"]\n",
    "X_train   = train_df.drop(columns=[\"transaction_id\", \"date\", \"target\"])\n",
    "\n",
    "# 1️⃣  sparse one-hot/scaled  →  2️⃣  SVD  →  dense 200-col\n",
    "X_train_sparse   = base_pre.fit_transform(X_train)\n",
    "scaler = StandardScaler()\n",
    "X_train_reduced  = svd.fit_transform(X_train_sparse)\n",
    "X_train_scaled   = scaler.fit_transform(X_train_reduced)\n",
    "\n",
    "iso = IsolationForest(\n",
    "        n_estimators=300,\n",
    "        contamination=0.10,         # trim 10 % of the data\n",
    "        max_samples=\"auto\",\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE\n",
    ")\n",
    "iso.fit(X_train_scaled)\n",
    "\n",
    "# Keep only normal points for AE training\n",
    "mask_keep   = iso.predict(X_train_scaled) == 1\n",
    "X_ae_dense  = scaler.transform(X_train_reduced[mask_keep]).astype(\"float32\")\n",
    "print(\"AE training set shape:\", X_ae_dense.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea285b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Build train / validation datasets  (FP16 + on-disk cache)\n",
    "# ---------------------------------------------------------------------------\n",
    "X_train, X_val = train_test_split(\n",
    "    X_ae_dense, test_size=0.20, random_state=RANDOM_STATE)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def make_ds(X, cache_path):\n",
    "    X = X.astype(\"float16\")                       # ½ the size of FP32\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, X))\n",
    "    ds = ds.cache(cache_path)                     # on-disk, not RAM\n",
    "    ds = ds.shuffle(buffer_size=min(20_000, len(X)),\n",
    "                    seed=RANDOM_STATE)\n",
    "    ds = ds.batch(BATCH)                          # no drop_remainder → smaller tail batch\n",
    "    return ds.prefetch(AUTOTUNE)\n",
    "\n",
    "ds_train = make_ds(X_train, CACHE_TRAIN)\n",
    "ds_val   = make_ds(X_val,   CACHE_VAL)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Auto-encoder architecture  (unchanged)\n",
    "# ---------------------------------------------------------------------------\n",
    "n_in = X_ae_dense.shape[1]\n",
    "enc_units      = [256, 128, 64, 32]\n",
    "bottleneck_dim = 16\n",
    "dec_units      = [32, 64, 128, 256]\n",
    "\n",
    "inp = layers.Input(shape=(n_in,))\n",
    "x   = layers.GaussianNoise(0.02)(inp)\n",
    "\n",
    "for u in enc_units:\n",
    "    x = layers.Dense(u, kernel_regularizer=tf.keras.regularizers.l2(1e-5))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(alpha=0.1)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "x = layers.Dense(bottleneck_dim,\n",
    "                 kernel_regularizer=tf.keras.regularizers.l2(1e-5))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "for u in dec_units:\n",
    "    x = layers.Dense(u, kernel_regularizer=tf.keras.regularizers.l2(1e-5))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "out = layers.Dense(n_in, activation=\"linear\", dtype=\"float32\")(x)\n",
    "\n",
    "ae = models.Model(inp, out)\n",
    "\n",
    "lr_sched = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    1e-3, decay_steps=2_000, decay_rate=0.90)\n",
    "opt = tf.keras.optimizers.AdamW(learning_rate=lr_sched, weight_decay=1e-5)\n",
    "\n",
    "ae.compile(optimizer=opt,\n",
    "           loss=losses.Huber(delta=1.0),\n",
    "           run_eagerly=False)\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    patience=12, restore_best_weights=True, verbose=1)\n",
    "\n",
    "history = ae.fit(\n",
    "    ds_train,\n",
    "    epochs=300,\n",
    "    validation_data=ds_val,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee39accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute AE threshold (99.99 percentile)\n",
    "val_size  = int(0.1 * len(X_ae_dense))\n",
    "recon_val = ae.predict(X_ae_dense[:val_size], batch_size=512, verbose=0)\n",
    "err_val   = np.mean(np.square(recon_val - X_ae_dense[:val_size]), axis=1)\n",
    "\n",
    "THRESH = np.percentile(err_val, 99.99)\n",
    "print(f\"Reconstruction-error threshold = {THRESH:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1601440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on chatgpt_data set\n",
    "test_csv = \"assets/chatgpt_data/transactions_o3_2_test.csv\"\n",
    "test_lbl = None   # set path if you have labels\n",
    "\n",
    "test_df   = load_and_prepare(test_csv, test_lbl)\n",
    "has_label = \"target\" in test_df.columns\n",
    "if has_label:\n",
    "    y_test = test_df[\"target\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"transaction_id\", \"date\"] +\n",
    "                                ([\"target\"] if has_label else []))\n",
    "\n",
    "# Stage 1 — IF\n",
    "X_test_sparse   = base_pre.transform(X_test)\n",
    "X_test_reduced  = svd.transform(X_test_sparse)\n",
    "X_test_scaled   = scaler.transform(X_test_reduced)\n",
    "\n",
    "if_flag = (iso.predict(X_test_scaled) == -1).astype(int)\n",
    "\n",
    "# Stage 2 — AE\n",
    "recon_test = ae.predict(X_test_scaled.astype(\"float32\"),\n",
    "                        batch_size=1024, verbose=0)\n",
    "err_test   = np.mean(np.square(recon_test - X_test_scaled), axis=1)\n",
    "ae_flag    = (err_test > THRESH).astype(int)\n",
    "\n",
    "y_pred = (if_flag | ae_flag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a8c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "if has_label:\n",
    "    print(\"Accuracy :\",  accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision:\",  precision_score(y_test, y_pred, zero_division=0))\n",
    "    print(\"Recall   :\",  recall_score(y_test, y_pred))\n",
    "    print(\"F1       :\",  f1_score(y_test, y_pred))\n",
    "\n",
    "    cm   = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=[\"No Fraud\", \"Fraud\"])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix — chatgpt_data\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Predictions generated (no ground-truth labels supplied).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pwc-autoencoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
